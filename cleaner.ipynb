{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T09:38:20.730841Z",
     "start_time": "2024-12-19T09:38:20.287660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import emoji\n",
    "import csv\n",
    "from spacy.pipeline.lemmatizer import lemmatizer_score\n",
    "\n",
    "dataset = pd.read_csv(\"./spam.csv\")\n",
    "print(dataset.describe())\n",
    "dataset = dataset.dropna(axis=1)\n",
    "dataset.head(10)\n",
    "\n",
    "file_to_write = open(\"./spam_cleaned.csv\", \"w\")\n",
    "new_csv = csv.writer(file_to_write)\n",
    "new_csv.writerow([\"label\", \"text\", 'len'])\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ],
   "id": "bf24a5ffd1310510",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          v1                      v2  \\\n",
      "count   5572                    5572   \n",
      "unique     2                    5169   \n",
      "top      ham  Sorry, I'll call later   \n",
      "freq    4825                      30   \n",
      "\n",
      "                                               Unnamed: 2  \\\n",
      "count                                                  50   \n",
      "unique                                                 43   \n",
      "top      bt not his girlfrnd... G o o d n i g h t . . .@\"   \n",
      "freq                                                    3   \n",
      "\n",
      "                   Unnamed: 3 Unnamed: 4  \n",
      "count                      12          6  \n",
      "unique                     10          5  \n",
      "top      MK17 92H. 450Ppw 16\"    GNT:-)\"  \n",
      "freq                        2          2  \n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T09:38:20.735031Z",
     "start_time": "2024-12-19T09:38:20.733316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filter unwanted tokens, special chars, numbers, and extra spaces\n",
    "# also remove stop words, as well as converting to lowercase\n",
    "# it will also remove emoticon as it is a punctuation\n",
    "def clean_token(doc):\n",
    "    cleaned_tokens = []\n",
    "    for token in doc:\n",
    "        if (not token.is_punct and not token.is_space\n",
    "                and not token.is_stop and not token.is_quote and token.is_ascii and not token.like_num):\n",
    "            # lemmatize the token\n",
    "            lemmatized_token = token.lemma_\n",
    "            cleaned_tokens.append(lemmatized_token.replace(\"--\", \"\").removesuffix(\"-\").removeprefix(\"-\").lower())\n",
    "    return cleaned_tokens"
   ],
   "id": "a69fc1c0ca81328d",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T09:38:20.815144Z",
     "start_time": "2024-12-19T09:38:20.783281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def remove_non_english_words(tokens):\n",
    "    return [token for token in tokens if token in words]"
   ],
   "id": "d5a42e2cec249e37",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/khantzawhein/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T09:38:31.000941Z",
     "start_time": "2024-12-19T09:38:20.877819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_text(text):\n",
    "    # Remove HTML like tags\n",
    "    regex_remove_html = re.compile(r'<.*?>')\n",
    "    text = regex_remove_html.sub('', text)\n",
    "    # convert unicode emojis to words\n",
    "    text = emoji.demojize(text)\n",
    "    doc = nlp(text)\n",
    "    cleaned_sentences = []\n",
    "    for sent in doc.sents:\n",
    "        cleaned_tokens = clean_token(sent)\n",
    "        cleaned_tokens = remove_non_english_words(cleaned_tokens)\n",
    "        cleaned_sentences.append(\" \".join(cleaned_tokens))\n",
    "    return \" \".join(cleaned_sentences)\n",
    "\n",
    "for index, review in enumerate(dataset['v2']):\n",
    "    processed_text = process_text(review)\n",
    "    new_csv.writerow([dataset.get('v1')[index], processed_text, len(dataset.get('v2')[index]) / 1000])\n",
    "    # new_csv.writerow([[int(label == \"spam\") for label in dataset[\"v1\"]][index], processed_text, len(dataset.get('v2')[index]) / 1000])"
   ],
   "id": "671ab2e23f38a7b1",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T09:38:31.047832Z",
     "start_time": "2024-12-19T09:38:31.046064Z"
    }
   },
   "cell_type": "code",
   "source": "file_to_write.close()",
   "id": "9f201e7559ed2ba1",
   "outputs": [],
   "execution_count": 98
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
